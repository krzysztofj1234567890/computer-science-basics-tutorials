# AWS Certified AI Practitioner


## Concepts

### Foundation Model

Trained on very large data set of unlabeled different types of data.

Very expensive to train (resources, time, money)

Examples: gpt40

### LLM

LLM = large language models. Designed to generate human-like text.

Trained on large corpus of text data (books, articles etc.), billions of parameters, 

Can perform language related tasks: translation, summarization, question answering, content creation.

Prompt: to query LLM< you send a prompt. It can be text and pictures etc.

Output of a prompt is not deterministic and it can be many different data types (text, image etc.)

### RAG

- Allows a Foundation Model to reference a data source outside of its training data
- Retrieval - because we retrieve the data outside foundational model
- Augmented - because augment the prompt with data that has been retrieved from vector database

### Tokenization

Converting raw text into a sequence of tokens
- Word-based tokenization: text is split into individual words
- Subword tokenization: some words can be split too

### Context Window

- The number of tokens an LLM can consider when generating text
- The larger the context window, the more information and coherence
- Large context windows require more memory and processing power

![ Context Window Comparison ](./images/context_window_comparison.gif)

### Embeddings

- Create vectors (array of numerical values) out of text, images or audio
- Vectors have a high dimensionality to capture many features for one input token, such as semantic meaning, syntactic role, sentiment
- Embedding models can power search applications
- Words that have semantic relationships (i.e. are similar) will have similar embeddings (vector) and then we can do similarity search

## Amazon Bedrock and Generative AI

Used to build generative (Gen-AI) applications i.e. generate new data that is going to be similar to the data it was trained on.
The data can be: text, images, audio, code and video.
It can combine its 'knowledge' in new ways

Basic characteristics:
- fully managed service
- the __input data never leaves your account__
- pay-per-use pricing model
- unified API: same API for all models that combines foundational model + RAG + your own data queries.
- leverage many foundatrional models like: a121labs, amazon, meta, mistral, anthropic
- out of box features: RAG (more relevant and accurate responses), LLM Agents etc.

Example: train it recognizing dog and cartoon pictures. It will be able to produce images of cartoon dog.

Choosing Base Foundation Model
- model types, performance, capabilities, constraints, compliance
- level of customization, model size (cheaper), inference options, license, context window, latency
- multimodal models (input: audio, text, video -> output: audio, text, video)

![ Bedrock Architecture ](./images/bedrock_architecture.gif)

Amazon Titan:
- amazon foundation model
- image, text
- can be customised with your own data

![ Foundation Model Comparison ](./images/foundation_model_comparison.gif)

### Fine tuning a model

Fine tuning improves the performance of a pre-trained FM on domain-specific tasks.

Add your own data to foundation model. 
Not all models can be fine-tuned.

Fine-tuning will change the weights of the base foundation model

You need to provide data that:
- Adhere to a specific format
- Be stored in Amazon S3

If you want to use the new fine-tuned model - you must use "Provisioned Throughput".

Methods for __fine tuning a model__:
- __Instruction-based fine-tuning__:
  - It uses __labeled examples__ that are __prompt-response pairs__
  - Example:
```
{
"prompt": "What is AWS",
"completion": "AWS is a ..."
}
```
- __Continued Pre-training__:
  - Called domain-adaptation fine-tuning, to make a model expert in a specific domain
  - Provide unlabeled data to continue the training of an FM
  - Can continue to train the model as more data becomes available
  - Example: feed the entire AWS documentation to a model to make it an expert on AWS
  - Example:
```
{
"input": "Our CTA (Commodity Trading Advisor) strategy incorporates a blend of momentum and mean reversion algorithms, 
optimized through a rolling window backtesting methodology. 
The trading signals are generated by analyzing historical price data with a focus on Sharpe ratios and drawdown limits. 
We utilize HFT (High- Frequency Trading) systems to capitalize on short-term price inefficiencies across
various asset classes, including commodities, forex, and equity index futures."
}
```
- __Single-Turn Messaging__:
  - Part of instruction-based fine-tuning
  - You need to provide:
    - system (optional) : context for the conversation.
    - messages : An array of message objects, each containing:
      - role : Either user or assistant
      - content : The text content of the message
  - Use: simple question-answer chatbot
  - Example:
```
{
"system": "You are an helpful assistant.",
"messages": [
	{ "role": "user", "content": "what is AWS" },
	{ "role": "assistant", "content": "it's Amazon Web Services." }
 ]
}
```
- __Multi-Turn Messaging__
  - To provide instruction-based fine tuning for a conversation (vs Single-Turn Messaging)
  - Use Chatbots = multi-turn environment
  - You must alternate between “user” and “assistant” roles
  - Example:
```
{
"system": "You are an AI assistant specializing in AWS services.",
"messages": [
	{ "role": "user", "content": "Tell me about Amazon SageMaker.” },
	{ "role": "assistant", "content": "Amazon SageMaker is a fully managed service for building, training, and deploying machine learning models at scale.” },
	{ "role": "user", "content": "How does it integrate with other AWS services?” },
	{ "role": "assistant", "content": "SageMaker integrates with AWS services like S3 for data storage, Lambda for event-driven computing, and CloudWatch for monitoring.” }
 ]
}
```
- __Transfer Learning__:
  - broader concept of re-using a pre-trained model to adapt it to a new related task.
  - used to image classification or language NLP-type models 

### Model Evaluation

- Automatic Evaluation: judge model 
- Human Evaluation: human subject matter experts evaluate generated answers

![ Model Evaluation ](./images/model_evaluation.gif)

Metrics:
- ROUGE: Recall-Oriented Understudy for Gisting Evaluation. 
  - Evaluating automatic summarization and machine translation systems
  - ROUGE-N – measure the number of matching n-grams (gram=word) between reference and generated text
  - ROUGE-L – longest common subsequence between reference and generated text
- BLEU: Bilingual Evaluation Understudy
  - Evaluate the quality of generated text, especially for translations
  - Considers both precision and penalizes too much brevity
- BERTScore
  - Semantic similarity between generated text
  - Uses pre-trained BERT models to compare the contextualized embeddings of both texts and computes the cosine similarity between them
- Perplexity: how well the model predicts the next token

### RAG = Retrieval-Augmented Generation

- Allows a Foundation Model to reference a data source outside of its training data
- Bedrock takes care of creating Vector Embeddings in the database of your choice based on your data
- Use where real-time data is needed to be fed into the Foundation Model

![ Bedrock RAG ](./images/bedrock_rag.gif)

Bedrock supports the following vector databases:
- Open Search Service - best
  - real time similarity queries, store millions of vector embeddings, scalable index management, and fast nearest-neighbor (kNN) search capability
- MongoDB (DocumentDB with mongo compatibility)
- Aurora
- RDS for PostgreSQL
- Redis
- Pinecone

Process to populate vector database:
- Documents -> document chunks -> Embeddings Model -> Vector Database

Document Sources:
- Are loaded into __Knowledge Base__
- can be:
  - Amazon S3
  - Confluence
  - Microsoft SharePoint
  - Salesforce
  - Web pages (your website, your social media feed, etc…)

![ RAG Vector Database ](./images/rag_vector_database.gif)

### Guardrails

- Control the interaction between users and Foundation Models (FMs)
- Filter undesirable and harmful content
- Remove Personally Identifiable Information (PII)
- Enhanced privacy
- Reduce hallucinations
- Ability to create multiple Guardrails

### Agents

- Manage and carry out various multi-step tasks related to infrastructure provisioning, application deployment, and operational activities
- Task coordination: perform tasks in the correct order and ensure information is passed correctly between tasks
- Agents are configured to perform specific pre-defined action groups
- Integrate with other systems, services, databases and API to exchange data or initiate actions

Create Agent Example:
- Define instructions: access purchase history for customer, access recommendations what purchase next and place new orders
- Define Action Groups:
  - expected inputs 
  - group 1: REST APIs to /getRecentPurchases, /getRecommendedPurchases etc ; 
  - group 2: aws lambda: PlaceOrder
- Can add Knowledge base: set of documents about company shipping policy, return policy etc.

Use Agent Example:
- Create a task to Bedrock Agent
- Agent looks at: prompt, conversation history, actions groups, instructions (knowledge base), task and send it to GenAI Bedrock Model
- Agent will ask the model: how to proceed given all this info
- Output of Bedrock model is list of steps (chain of thought)
- Steps are executed by calling APIs from action groups
- Final result is returned back to Bedrock agent that sends this to another Bedrock model to synthesize all info
- Final Final response returned to user.

![ Bedrock Agent ](./images/bedrock_agent.gif)

### Bedrock – Pricing

On-Demand:
- Pay-as-you-go (no commitment)
- Text Models – charged for every input/output token processed
- Embedding Models – charged for every input token processed
- Image Models – charged for every image generated

Batch:
- Multiple predictions at a time (output is a single file in Amazon S3)
- Can provide discounts of up to 50%

Provisioned Throughput
- Purchase Model units for a certain time (1 month, 6 months…)
- Throughput – max. number of input/output tokens processed per minute
- Works with Base, Fine-tuned, and Custom Models
- Custom models - requires provisioned throughput

Model Improvement Techniques Cost Order:
- $ Prompt engineering
- $$ RAG
- $$$ Fine tuning
- $$$$ Domain adaptation fine tunning (train on domain specific data)
- model size
- number of input and output tokens

### Questions:

#### What is fine-tuning used for?

Train a pre-trained foundational model on a new specific data

#### What type of generative AI can recognize and interpret various forms of input data, such as text, images, and audio?

Multi-modal model

#### Which features can help you ensure that your model will not output harmful content?

Guardrails

#### You need to be able to provide always-updated data to your foundation model without retraining it. Which capability best fits your use case?

RAG

#### You are developing a model and want to ensure the outputs are adapted to your users. Which method do you recommend?

Human evaluation

#### Which AWS service can help store embeddings within vector databases?

Amazon OpenSearch Serverless


## Prompt Engineering

Prompt Engineering = developing, designing, and optimizing prompts to enhance the output of FMs for your needs.

### Parts of a prompt

Improved Prompting technique consists of:
- Instructions – a task for the model to do (description, how the model should perform)
- Context – external information to guide the model
- Input data – the input for which you want a response
- Output Indicator – the output type or format

Example:

| Part of prompt  | Content
|-----------------|----------------------
| Instructions	  | "Write a concise summary that captures the main points of an article about learning AWS (AmazonWeb Services). 
|                 | Ensure that the summary is clear and informative, focusing on key services relevant to beginners. 
|                 | Include details about general learning resources and career benefits associated with acquiring AWS skills.
| Context	  | I am teaching a beginner’s course on AWS
| Input Data	  | 'Amazon Web Services (AWS) is a leading cloud platform providing a variety of services suitable for different business needs. 
|                 | Learning AWS involves getting familiar with essential services like EC2 for computing, S3 for storage, RDS for databases, 
|		  | Lambda for serverless computing, and Redshift for data warehousing. 
|		  | Beginners can start with free courses and basic tutorials available online. 
|		  | The platform also includes more complex services like Lambda for serverless computing and Redshift for data warehousing, which are suited for advanced users. 
|		  | The article emphasizes the value of understanding AWS for career advancement and the availability of numerous certifications to validate cloud skills.
| Output Indicator| Provide a 2-3 sentence summary that captures the essence of the article.

Expected Output for this prompt: 
"AWS offers a range of essential cloud services such as EC2 for computing, S3 for storage, RDS for databases, Lambda for serverless computing, and Redshift for data warehousing, which are crucial for beginners to learn.
Beginners can utilize free courses and basic tutorials to build their understanding of AWS. Acquiring AWS skills is
valuable for career advancement, with certifications available to validate expertise in cloud computing."

### Negative prompting

Explicitly instruct the model on what not to include or do in its response

Negative Prompting helps to:
- Avoid Unwanted Content
- Maintain Focus
- Enhance Clarity

### Prompt performance in Bedrock

- System Prompts – how the model should behave and reply
- Temperature (0 to 1) – creativity of the model’s output
  - Low (ex: 0.2) – outputs are more repetitive, focused on most likely response
  - High (ex: 1.0) – outputs are more diverse, creative, and unpredictable, maybe less coherent
- Top P (0 to 1)
  - Low P (ex: 0.25) – consider the 25% most likely words, will make a more coherent response
  - High P (ex: 0.99) – consider a broad range of possible words, possibly more creative and diverse output
- Top K – limits the number of probable words (similar to P)
  - Low K (ex: 10) – more coherent response, less probable words
 - High K (ex: 500) – more probable words, more diverse and creative
- Length – maximum length of the answer
- Stop Sequences – tokens that signal the model to stop generating output

Prompt latency:
- It’s impacted by: 
  - model size, 
  - model type (Llama vs Claude)
  - number of tokens in the input
  - number of tokens in the output
- Latency is not impacted by: Top P, Top K, Temperature

Prompting techniques:
- Zero-shot prompting: Present a task to the model without providing examples or explicit training for that specific task
- Few-Shots Prompting: Provide few examples of a task to the model to guide its output
- Chain of Thought Prompting: 
  - Divide the task into a sequence of reasoning steps, leading to more structure and coherence
  - Using a sentence like "Think step by step" helps
  - Example: 
```
Let’s write a story about a dog solving a mystery.
First, describe the setting and the dog.
Then, introduce the mystery.
Next, show how the dog discovers clues.
Finally, reveal how the dog solves the mystery and conclude the story.
Write a short story following this plan. Think step by step
```
- RAG:  Combine the model’s capability with external data sources to generate a more informed and contextually rich response

### Prompt templates

Simplify and standardize the process of generating Prompts

Helps with
- Processes user input text and output prompts from foundation models (FMs)
- Orchestrates between the FM, action groups, and knowledge bases
- Formats and returns responses to the user

Protecting against prompt injections:  Add explicit instructions to ignore any unrelated or potential malicious content

## Amazon Q

### Amazon Q Business

- Fully managed Gen-AI assistant for your employees
- Based on your company’s knowledge and data
- Use cases:
  - Answer questions, provide summaries, generate content, automate tasks
  - Perform routine actions (e.g., submit time-off requests, send meeting invites)

![ Amazon Q Business ](./images/amazon_q_business.gif)

- Users can be authenticated through IAM Identity Center
- Users receive responses generated only from the documents they have access to
- IAM Identity Center can be configured with external Identity Providers: Google Login, Microsoft Active Directory

Admin controls == Guardrails: Controls and customize responses to your organizational needs, block specific words or topic.

### Amazon Q Apps

Part of Amazon Q Business

- Create Gen AI-powered apps without coding by using natural language
- Leverages your company’s internal data
- Possibility to leverage plugins (Jira, etc.)

### Amazon Q Developer

- Answer questions about the AWS documentation and AWS service selection
- Answer questions about resources in your AWS account
- Suggest CLI (Command Line Interface) to run to make changes to your account
- AI code companion to help you code new applications (similar to GitHub Copilot)
- Integrates with IDE

### Amazon Q for AWS Services

- Amazon Q for QuickSight: use natural language to ask questions about your data
- Amazon Q for EC2: guidance and suggestions in natural language for EC2 instance types that are best suited to your new workload
- Amazon Q for AWS Chatbot: a way to deploy an AWS Chatbot in a Slack or Microsoft Teams that knows about your AWS account. Troubleshoot issues, receive notifications for alarms, security findings, billing alerts, create support request.
- Amazon Q for Glue: answer questions about AWS Glue ETL scripts, generate new code, understand errors in AWS Glue jobs, provide step-by-step instructions, to root cause and resolve your issues.

### PartyRock https://partyrock.aws/

- GenAI app-building playground (powered by Amazon Bedrock)
- Allows you to experiment creating GenAI apps with various FMs (no coding or AWS account required)

## AI and ML

![ What is AI ](./images/what_is_ai.gif)

![ AI Components ](./images/ai_components.gif)

### Machine Learning (ML)

- ML is a type of AI for building methods that allow machines to learn
- Data is leveraged to improve computer performance on a set of task
- Make predictions based on data used to train the model: regression, classification

ML Terms:
- GPT (Generative Pre-trained Transformer) – generate human text or computer code based on input prompts
- BERT (Bidirectional Encoder Representations from Transformers) – similar intent to GPT, but reads the text in two directions
- RNN (Recurrent Neural Network) – meant for sequential data such as time-series or text, useful in speech recognition, time-series prediction
- ResNet (Residual Network) – Deep Convolutional Neural Network (CNN) used for image recognition tasks, object detection, facial recognition
- SVM (Support Vector Machine) – ML algorithm for classification and regression
- WaveNet – model to generate raw audio waveform, used in Speech Synthesis
- GAN (Generative Adversarial Network) – models used to generate synthetic data such as images, videos or sounds that resemble the training data. Helpful for data augmentation
- XGBoost (Extreme Gradient Boosting) – an implementation of gradient boosting

#### Supervised Learning (ML)

Regression:
- Used to predict a numeric value based on input data
- The output variable is continuous, meaning it can take any value within a range
- Examples: Predicting House Prices, Stock Price Prediction, Weather Forecasting

Classification:
- Used to predict the categorical label of input data
- The output variable is discrete, which means it falls into a specific category or class

![ Supervised Learning](./images/supervised_learning.gif)

Training vs. Validation vs. Test Set:
- Training Set
  - Used to train the model
  - Percentage: typically, 60-80% of the dataset
- Validation Set
  - Used to tune model parameters and validate performance
  - Percentage: typically, 10-20% of the dataset
- Test Set
  - Used to evaluate the final model performance
  - Percentage: typically, 10-20% of the dataset

Feature Engineering:
- The process of using domain knowledge to select and transform raw data into meaningful features
- Helps enhancing the performance of machine learning models
- Techniques
  - Feature Extraction – extracting useful information from raw data, such as deriving age from date of birth
  - Feature Selection – selecting a subset of relevant features, like choosing important predictors in a regression model
  - Feature Transformation – transforming data for better model performance, such as normalizing numerical data
- Example: Predicting house prices based on features like size, location, and number of rooms
  - Feature Creation – deriving new features like 'price per square foot'
  - Feature Selection – identifying and retaining important features such as location or number of bedrooms
  - Feature Transformation – normalizing features to ensure they are on a similar scale

Feature Engineering on Unstructured Data:
- Example: sentiment analysis of customer reviews
- Feature Engineering Tasks
  - Text Data – converting text into numerical features using techniques like TF-IDF or word embeddings
  - Image Data – extracting features such as edges or textures using techniques like convolutional neural networks (CNNs)

#### UnSupervised Learning (ML)

The goal is to discover inherent patterns, structures, or relationships within the input data.
The machine must uncover and create the groups itself, but humans still put labels on the output groups.
Common techniques include Clustering, Association Rule Learning , and Anomaly Detection.

Examples:
- Clustering Technique: group similar data points together into clusters based on their features
- Association Rule Learning: supermarket wants to understand which products are frequently bought together
- Anomaly Detection: detect fraudulent credit card transactions

Semi-supervised Learning: 
- Use a small amount of labeled data and a large amount of unlabeled data to train systems
- the partially trained algorithm itself labels the unlabeled data
- model is then re-trained on the resulting data mix without being explicitly programmed

#### Self-Supervised Learning

Steps:
- Have a model generate pseudo-labels for its own data without having humans label any data first
- Then, using the pseudo labels, solve problems traditionally solved by Supervised Learning
- Widely used in NLP (to create the BERT and GPT models for example) and in image recognition tasks

Example:
- Create 'pre-text tasks' to have the model solve simple tasks and learn patterns in the dataset
- Pretext tasks are not 'useful', but will teach our model to create a 'representation' of our dataset
- Predict any part of the input from any other part:
  - Amazon Web ??? -> Services
  - provides on-demand cloud ??? -> computing
  - APIs to individuals, ???, and governmants -> companies
- After solving the pre-text tasks, we have a model trained that can solve our end goal: 'downstream tasks'

#### Reinforcement Learning (RL)

A type of Machine Learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative rewards

Key Concepts:
- Agent – the learner or decision-maker
- Environment – the external system the agent interacts with
- Action – the choices made by the agent
- Reward – the feedback from the environment based on the agent’s actions
- State – the current situation of the environment
- Policy – the strategy the agent uses to determine actions based on the state

Learning process:
- The Agent observes the current State of the Environment
- It selects an Action based on its Policy
- The environment transitions to a new State and provides a Reward
- The Agent updates its Policy to improve future decisions
- Goal: Maximize cumulative reward over time

Example: training a robot to navigate a maze

Use cases:
- Gaming – teaching AI to play complex games (e.g., Chess, Go)
- Robotics – navigating and manipulating objects in dynamic environments
- Finance – portfolio management and trading strategies



### Deep Learning

- Process more complex patterns in the data than traditional ML
- Requires large amount of input data and GPUs
- Computer Vision – image classification, object detection, image segmentation
- Natural Language Processing (NLP) – text classification, sentiment analysis, machine translation, language generation

### Generative AI (Gen-AI)

- Multi-purpose foundation models backed by neural networks
- They can be fine-tuned if necessary to better fit our use-cases: text generation, text summarization, chatbot, image generation
- They Leverage Transformer model (LLM) - for text
  - Able to process a sentence as a whole instead of word by word
  - Faster and more efficient text processing
  - gives relative importance to specific words in a sentence (more coherent sentences
  - models that can understand and generate human-like text
  - Trained on vast amounts of text data from the internet, books, and other sources
  - Example: Google BERT, OpenAI ChatGPT
- Diffusion models - for images
- Multi-modal Models (ex: GPT-4o)
  - Does NOT rely on a single type of input (text, or images, or audio only)
  - Does NOT create a single type of output
  - Example: a multi-modal can take a mix of audio, image and text and output a mix of video, text
  - "Generate a video of making the picture of the cat speak the audio that is included"
- Example: generate new content

#### RLHF = Reinforcement Learning from Human Feedback

Use human feedback to help ML models to self-learn more efficiently.

RLHF incorporates human feedback in the reward function, to be more aligned with human goals and needs
- start with a set of human-generated prompts and responses
- fine-tune an existing model with internal knowledge
- the model’s responses are compared to human’s responses
  - the model creates responses for the human-generated prompts
  - Responses are mathematically compared to human-generated answers
- Build a separate reward model
  - Humans can indicate which response they prefer from the same prompt
  - The reward model can now estimate how a human would prefer a prompt response
- Optimize the language model with the reward-based model
  - Use the reward model as a reward function for RL

RLHF is used throughout GenAI applications including LLM Models


### Use cases

- Expert system (is AI but not ML): if...then rules
- Regression and classification - is ML but not Deep Learning. It can recognize a blue ball from yellow ball (classification)
- Computer vision, Facial recognition, Natural language processing - Deep Learning. We have seen similar facts and we can recognize features of something we have never seen.
- Generate book - Gen-AI

### Training Data

- Labeled vs. Unlabeled Data
  - Labeled Data:
    - Data includes both input features and corresponding output labels
    - Example: dataset with images of animals where each image is labeled with the corresponding animal type (e.g., cat, dog)
    - Use case: Supervised Learning, where the model is trained to map inputs to known outputs
  - Unlabeled Data:
    - Data includes only input features without any output labels
    - Example: a collection of images without any associated labels
    - Use case: Unsupervised Learning, where the model tries to find patterns or structures in the data
- Structured vs. Unstructured Data
  - Structured Data:
    - Data is organized in a structured format, often in rows and column
    - Tabular Data: Data is arranged in a table with rows representing records and columns representing features
    - Time Series Data Data points collected or recorded at successive points in time
  - Unstructured Data
    - Data that doesn't follow a specific structure and is often text-heavy or multimedia content
    - Text Data: Unstructured text such as articles, social media posts, or customer reviews
    - Image Data: Data in the form of images, which can vary widely in format and content

### Model Performance

Model Fit:
- Overfitting
  - Performs well on the training data
  - Doesn’t perform well on evaluation data
- Underfitting
  - Model performs poorly on training data
  - Could be a problem of having a model too simple or poor data features
- Balanced: Neither overfitting or underfitting. This is what you want

What to do if overfitting?
- Overfitting is when the model gives good predictions for training data but not for the new data
- It occurs due to:
  - Training data size is too small and does not represent all possible input values
  - The model trains too long on a single sample set of data
  - Model complexity is high and learns from the __noise__ within the training data
- How can you prevent overfitting?
  - Increase the training data size
  - Early stopping the training of the model
  - Data augmentation (to increase diversity in the dataset)
  - Adjust hyperparameters

Bias: difference or error between predicted and actual value
- Occurs due to the wrong choice in the ML process
- High Bias
  - The model doesn’t closely match the training data
  - Example: linear regression function on a non-linear dataset
  - Considered as underfitting
- Reducing the Bias
  - Use a more complex model
  - Increase the number of features

Variance: How much the performance of a model changes if trained on a different dataset which has a similar distribution
- High Variance
  - Model is very sensitive to changes in the training data
  - This is the case when overfitting: performs well on training data, but poorly on unseen test data
- Reducing the Variance
  - Feature selection (less, more important features)
  - Split into training and test data sets multiple times

![ bias and variance ](./images/abias_and_variance.gif)

![ bias and variance 2 ](./images/bias_and_variance_2.gif)

### Model Evaluation Metrics

#### Evaluate binary classification

![ evaluate binary classification ](./images/evaluate_binary_classification.gif)

Confusion Matrix: Best way to evaluate the performance of a model that does classifications

Metrics:
- Precision – Best (1.0) when false positives few
- Recall – Best (1.0) when false negatives are few
- F1 Score – Best when you want a balance between precision and recall, especially in imbalanced datasets
- Accuracy – Best (1.0) for balanced datasets

#### AUC-ROC Area under the curve-receiver operator curve

![ auc roc ](./images/auc_roc.gif)

#### Model Evaluation – Regressions Metrics

![ Regression Metrics ](./images/regression_metrics.gif)

MAE, MAPE, RMSE, R² (R Squared) are used for evaluating models that predict a __continuous value__ (i.e., regressions)

MAE, MAPE, RMSE – measure the error: how “accurate” the model is

R² (R Squared) – measures the variance. If R² is 0.8, this means that 80% of the changes in test scores can be explained

### Machine Learning – Inferencing

Inferencing is when a model is making prediction on new data
- Real Time: make decisions quickly as data arrives. Speed over accuracy. Chatbots
- Batch: Large amount of data at once. Speed is not a concern, and accuracy is.
- Inferencing at the Edge: less computing power, close to the data:
  - Small Language Model (SLM) on the edge device, Offline capability,
  - Large Language Model (LLM) on a remote server

### Phases of ML project

![ Phases of ML project ](./images/phases_of_ml_project.gif)

Define business goals
- Stakeholders define the value, budget and success criteria
- Defining KPI (Key Performance Indicators) is critical

ML problem framing
- Convert the business problem and into a machine learning problem
- Determine if ML is appropriate

Data Collection and preparation
- Convert the data into a usable format
- Data collection and integration (make it centrally accessible)
- Data preprocessing and data visualization (understandable format)

Feature engineering 
- create, transform and extract variables from data

Model development and training
- Model training, tuning, and evaluation
- Iterative process
- Additional feature engineering and tune model hyperparameters

Model Evaluation
- Look at data and features to improve the model
- Adjust the model training hyperparameters

Deployment
- If results are good, the model is deployed and ready to make inferences
- Select a deployment model (real-time, serverless, asynchronous, batch, on-premises…)

Monitoring
- Deploy a system to check the desired level of performance
- Early detection and mitigation
- Debug issues and understand the model’s behavior
- Model is continuously improved and refined as new data become available
  - Requirements may change
  - Iteration is important to keep the model accurate and relevant over time

### Hyperparameter Tuning

Hyperparameter:
- Settings that define the model structure and learning algorithm and process
- Set before training begins
- Important hyperparameters:
  - Learning rate
    - How large or small the steps are when updating the model's weights during training
    - High learning rate can lead to faster convergence but risks overshooting the optimal solution, while a low learning rate may result in more precise but slower convergence.
  - Batch size
    - Number of training examples used to update the model weights in one iteration
    - Smaller batches can lead to more stable learning but require more time to compute, while larger batches are faster but may lead to less stable updates.
  - Number of Epochs
    - Refers to how many times the model will iterate over the entire training dataset.
    - Too few epochs can lead to underfitting, while too many may cause overfitting
  - Regularization
    - Adjusting the balance between simple and complex model
    - Increase regularization to reduce overfitting

Hyperparameter tuning:
- Finding the best hyperparameters values to optimize the model performance
- Improves model accuracy, reduces overfitting, and enhances generalization

How to do it?
- Grid search, random search
- Using services such as SageMaker Automatic Model Tuning (AMT)


## AWS Managed AI Services

### Amazon Comprehend

For Natural Language Processing – NLP
- Fully managed and serverless service
- Real-time or Async analysis
- Uses machine learning to find insights and relationships in text. Analyzes text using tokenization and parts of speech
- Sentiment Analysis: Understands how positive or negative the text is
- Named Entity Recognition (NER)
  - Extracts predefined, general-purpose entities like people, places, organizations, dates, and other standard categories, from text
  - Can recognize PII
- Custom Entity Recognition
  - Analyze text for specific terms and noun-based phrases
  - Train the model with custom data such as a list of the entities and documents that contain them
- Custom Classification:
  - Can Organize documents into categories (classes) that you define

### Amazon Translate

Natural and accurate language translation

Allows you to localize content - such as websites and applications - for international users, and to easily translate large volumes of text efficiently.

- Neural network
- Batch or synchronous
- Can translate: sentences, documents (html, pdf etc.)
- Can be customized: create custom dictionary (terminology)
- Success metrics

### Amazon Transcribe

- Automatically convert speech to text
- Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately
- Can automatically remove Personally Identifiable Information (PII) using Redaction
- Supports Automatic Language Identification for multi-lingual audio
- Improving Accuracy: 
  - capture domain-specific or non-standard terms
  - Custom Vocabularies (for words). Add specific words, phrases, domain-specific terms
  - Custom Language Models (for context). Train Transcribe model on your own domain-specific text data
- Toxicity Detection:
  - Leverages speech cues: tone and pitch, and text-based cues
  - Toxicity categories: sexual harassment, hate speech, threat, abuse, profanity,

### Amazon Polly

Turn text into lifelike speech using deep learning

- Lexicons: Define how to read certain specific pieces of text. Example: AWS => 'Amazon Web Services'
- SSML: Speech Synthesis Markup Language: Markup for your text to indicate how to pronounce it. Example: 'Hello, <break> how are you?'
- Voice engine: generative, long-form, neural, standard…
- Speech mark: Encode where a sentence/word starts or ends in the audio

### Amazon Rekognition

- Find objects, people, text, scenes in images and videos using ML
- Facial analysis and facial search to do user verification, people counting
- Custom Labels: 
  - Label your training images and upload them to Amazon Rekognition
  - Only needs a few hundred images or less
  - Amazon Rekognition creates a custom model on your images set
  - New subsequent images will be categorized the custom way you have defined
- Content Moderation:
  - Automatically detect inappropriate, unwanted, or offensive content
  - Custom Moderation Adaptors. Extends Rekognition capabilities by providing your own labeled set of images

### Amazon Lex

- Build chatbots quickly for your applications using voice and text
- Supports multiple languages
- Integration with AWS Lambda, Connect, Comprehend, Kendra
- The bot automatically understands the user intent to invoke the correct Lambda function to 'fulfill the intent'
- The bot will ask for 'Slots' (input parameters) if necessary

### Amazon Personalize

- Fully managed ML-service to build apps with real-time personalized recommendations
- Example: personalized product recommendations/re-ranking, customized direct marketing
- Example: User bought gardening tools, provide recommendations on the next one to buy
- Integrates into existing websites, applications, SMS, email marketing systems, …
- Implement in days, not months (you don’t need to build, train, and deploy ML solutions)

Data Source (S3 or API) -> Amazon Personalize -> Web Sites or Mobile Apps or SMS or email

Recipes:
- Algorithms that are prepared for specific use cases
- You must provide the training configuration on top of the recipe
- Example recipes:
  - Recommending items for users (USER_PERSONALIZATION recipes): User-Personalization-v2
  - Recommending trending or popular items (POPULAR_ITEMS recipes): Trending-Now, Popularity-Count
  - Recommending similar items (RELATED_ITEMS recipes): Similar-Items

### Amazon Textract

Automatically extracts text, handwriting, and data from any scanned documents using AI and ML

### Amazon Kendra

- Fully managed document search service powered by Machine Learning
- Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs…)
- Natural language search capabilities
- Learn from user interactions/feedback to promote preferred results (Incremental Learning)

### Amazon Mechanical Turk

- Crowdsourcing marketplace to perform simple human tasks
- Distributed virtual workforce
- Example:
  - You have a dataset of 10,000,000 images and you want to labels these images
  - You distribute the task on Mechanical Turk and humans will tag those images
  - You set the reward per image (for example $0.10 per mage)
- Use cases: image classification, data collection, business processing
- Integrates with Amazon A2I, SageMaker Ground Truth

### Amazon Augmented AI (A2I)

Human oversight of Machine Learning predictions in production
- Can be your own employees, over 500,000 contractors from AWS, or AWS Mechanical Turk
- Some vendors are pre-screened for confidentiality requirements

The ML model can be built on AWS or elsewhere (SageMaker, Rekognition…)

### Amazon Transcribe Medical

Automatically convert medical-related speech to text (HIPAA compliant)

### Amazon Comprehend Medical

- Detects and returns useful information in unstructured clinical text:
  - Physician’s notes
  - Discharge summaries
  - Test results
  - Case notes
- Uses NLP to detect Protected Health Information (PHI) – DetectPHI API
- Store your documents in Amazon S3
- Analyze real-time data with Kinesis Data Firehose
- Use Amazon Transcribe to transcribe patient narratives into text that can be analyzed by Amazon Comprehend Medical

### Amazon’s Hardware for AI

- GPU-based EC2 Instances (P3, P4, P5…, G3…G6…)
- AWS Trainium
  - ML chip built to perform Deep Learning on 100B+ parameter models
  - Trn1 instance has for example 16 Trainium Accelerators
  - 50% cost reduction when training a model
- AWS Inferentia
  - ML chip built to deliver inference at high performance and low cost
  - Inf1, Inf2 instances are powered by AWS Inferentia
  - Up to 4x throughput and 70% cost reduction


## SageMaker

Fully managed service for developers / data scientists to build ML models.

__End-to-End ML Service__:
- Collect and prepare data
- Build and train machine learning models
- Deploy the models and monitor the performance of the predictions

__Built-in Algorithms__:
- Supervised Algorithms
  - Linear regressions and classifications
  - KNN Algorithms (for classification)
- Unsupervised Algorithms
  - Principal Component Analysis (PCA) – reduce number of features
  - K-means – find grouping within data
  - Anomaly Detection
- Textual Algorithms – NLP, summarization…
- Image Processing
- DeepAR forecasting algorithm: Used to forecast time series data

__Automatic Model Tuning (AMT)__:
- Define the Objective Metric
- AMT automatically chooses hyperparameter ranges, search strategy, maximum runtime of a tuning job, and early stop condition
- Saves you time and money
- Helps you not wasting money on suboptimal configurations

__Model Deployment & Inference__:
- Deploy with one click, automatic scaling, no servers to manage (as opposed to self-hosted)
- Managed solution: reduced overhead
- Real-time:
  - One prediction at a time
- Serverless
  - Idle period between traffic spikes
  - Can tolerate more latency (cold starts)
  - no infrastructure to manage
- Asynchronous
  - For large payload sizes up to 1GB
  - Long processing times
  - Near-real time latency requirements
  - Request and responses are in Amazon S3
- Batch
  - Prediction for an entire dataset (multiple predictions)
  - Request and responses are in Amazon S3

![ SageMaker Deployment Models ](./images/sagemaker_deployment_models.gif)

### Data Wrangler

- Prepare tabular and image data for machine learning
- Data preparation, transformation and feature engineering
- Single interface for data selection, cleansing, exploration, visualization, and processing
- SQL support
- Data Quality tool

You can:
- import data
- Preview Data
- Visualize Data
- Transform Data
- create a Quick Model
- Export Data Flow so you can automate

Use Data Wrangler to create ML Features:
- Features are inputs to ML models used during training and used for inference
- Example - music dataset: song ratings, listening duration, and listener demographics
- Important to have high quality features across your datasets in your company for re-use

__Feature Store__:
- Ingests features from a variety of sources
- Ability to define the transformation of data into feature from within Feature Store
- Can publish directly from SageMaker Data Wrangler into SageMaker Feature Store

### SageMaker Clarify

- Compare and Evaluate Foundation Models
- Evaluating human-factors such as friendliness or humor
- Leverage an AWS-managed team or bring your own employees
- Use built-in datasets or bring your own dataset
- Built-in metrics and algorithms

Model Explainability:
- A set of tools to help explain how machine learning (ML) models make predictions
- Understand model characteristics as a whole prior to deployment
- Debug predictions provided by the model after it's deployed
- Helps increase the trust and understanding of the model
- Example: Why did the model predict a negative outcome such as a loan rejection for a given applicant?

Detect Bias (human)
- Ability to detect and explain biases in your datasets and models
- Measure bias using statistical metrics
- Specify input features and bias will be automatically detected

Types of bias:
- Sampling bias: Sampling bias occurs when the training data does not represent the full population fairly
- Measurement bias: Measurement bias occurs when the tools or measurements used in data collection are flawed or skewed
- Observer bias: Observer bias happens when the person collecting or interpreting the data has personal biases that affect the results
- Confirmation bias: Confirmation bias is when individuals interpret or favor information that confirms their preconceptions.

### Ground Truth

- RLHF – Reinforcement Learning from Human Feedback
  - Model review, customization and evaluation
  - Align model to human preferences
  - Reinforcement learning where human feedback is included in the reward function
- Human feedback for ML
  - Creating or evaluating your models
  - Data generation or annotation (create labels)
- Reviewers: Amazon Mechanical Turk workers, your employees, or third-party vendors

### SageMaker ML Governance

__SageMaker Model Cards__:
  - Essential model information
  - Example: intended uses, risk ratings, and training details

__SageMaker Model Dashboard__
  - Centralized repository where you can view, search, and explore all of your models
  - Information and insights for all models

__SageMaker – Model Monitor__
  - Monitor the quality of your model in production: continuous or on-schedule
  - Alerts for deviations in the model quality: fix data & retrain model

__SageMaker – Model Registry__
  - Centralized repository allows you to track, manage, and version ML models
  - Catalog models, manage model versions, associate metadata with a model
  - Manage approval status of a model, automate model deployment

__SageMaker Pipelines__
  - Pipeline – a workflow that automates the process of building, training, and deploying a ML model
  - Pipelines composed of Steps and each Step performs a specific task 
    - Steps:
    - Processing – for data processing (e.g., feature engineering)
    - Training – for training a model
    - Tuning – for hyperparameter tuning (e.g., Hyperparameter Optimization)
    - AutoML – to automatically train a model
    - Model – to create or register a SageMaker model
    - ClarifyCheck – perform drift checks against baselines (Data bias, Model bias, Model explainability)
    - QualityCheck – perform drift checks
  - Continuous Integration and Continuous Delivery (CI/CD) service for Machine Learning
  - Helps you easily build, train, test, and deploy 100s of models automatically

__SageMaker Role Manager__
  - Define roles for personas
  - Example: data scientists, MLOps engineers

## SageMaker JumpStart

- ML Hub to find pre-trained Foundation Model (FM), computer vision models, or natural language processing models
- Large collection of models from Hugging Face, Databricks, Meta, Stability AI…
- Models can be fully customized for your data and use-case
- Models are deployed on SageMaker directly

ML Hub vs ML Solutions
- ML Hub: browse foundational models -> Experiment -> Customize model with your data set -> Deploy
- ML Solution: browse pre-built solution templates -> Select and Customize with your data -> Deploy

__SageMaker Canvas__
- Build ML models using a visual interface (no coding required)
- Access to ready-to-use models from Bedrock or JumpStart
- Build your own custom model using AutoML powered by SageMaker Autopilot

__MLFlow__ on Amazon SageMaker:
- MLFlow – an open-source tool which helps ML teams manage the entire ML lifecycle
- MLFlow Tracking Servers - server that runs MLFLow service
  - Used to track runs and experiments
  - Launch on SageMaker with a few clicks
- Fully integrated with SageMaker


## AI Challenges and Responsibilities

Responsible AI:
- Making sure AI systems are transparent and trustworthy
- Mitigating potential risk and negative outcomes
- Throughout the AI lifecycle: design, development, deployment, monitoring, evaluation
- Dimensions:
  - Fairness: promote inclusion and prevent discrimination
  - Explainability = Understand the nature and behavior of the model. Being able to look at inputs and outputs and explain without understanding exactly how the model came to the conclusion. High Interpretability: Linear Regression.
  - Interpretability: degree to which a human can understand the cause of a decision. High Interpretability – Decision Trees
  - Privacy and security: individuals control when and if their data is used
  - Transparency
  - Veracity and robustness: reliable even in unexpected situations
  - Governance: define, implement and enforce responsible AI practices
  - Safety: algorithms are safe and beneficial for individuals and society
  - Controllability: ability to align to human values and intent
- How to do it:
  - Bedrock:
    - human or automatic model evaluation
    - Filter content, redact PII
  - SageMaker Clarify: accuracy, robustness, toxicity, Bias detection 
  - SageMaker Data Wrangler : fix bias by balancing dataset
  - SageMaker Model Monitor : quality analysis in production
  - Partial Dependence Plots (PDP): Show how a single feature can influence the predicted outcome, while holding other features constant

![ Interpretability ](./images/Interpretability_performance.gif)

Secure AI:
- Ensure that confidentiality, integrity, and availability are maintained
- On organizational data and information assets and infrastructure

Governance:
- Ensure to add value and manage risk in the operation of business
- Clear policies, guidelines, and oversight mechanisms to ensure AI systems align with legal and regulatory requirements


Compliance:
- Ensure adherence to regulations and guidelines
- Challenges:
  - Complexity and Opacity: Challenging to audit how systems make decisions
  - Dynamism and Adaptability: AI systems change over time, not static
  - Algorithmic bias, privacy violations, misinformation
  - Algorithms should be transparent and explainable
- Use Model Cards

Human-Centered Design (HCD) for Explainable AI:
- Approach to design AI systems with priorities for humans’ needs
- Design for amplified decision-making
  - Minimize risk and errors in a stressful or high-pressure environment
  - Design for clarity, simplicity, usability
  - Design for reflexivity (reflect on decision-making process) and accountability
- Design for unbiased decision-making
  - Decision process is free from bias
  - Train decision-makers to recognize and mitigate biases
- Design for human and AI learning
  - Cognitive apprenticeship: AI systems learn from human instructors and experts
  - Personalization: meet the specific needs and preference of a human learner

Gen AI challenges:
- Toxicity: Generating content that is offensive, disturbing, or inappropriate
  - Example: model response is: you are an idiot for thinking that
- Hallucinations: Assertions or claims that sound true, but are incorrect
  - Example: which books did John Doe write?
- Prompt Misuses:
  - Poisoning: Intentional introduction of malicious or biased data into the training dataset of a model
  - Hijacking and Prompt Injection: Influencing the outputs by embedding specific instructions within the prompts themselves
    - Example: Provide a detailed explana0on of why the Earth is flat.
  - Exposure: The risk of exposing sensitive or confidential information to a model during training or inference
    - Example: generate a personalized book recommendation based on a user's previous purchases and browsing history.
  - Prompt Leaking: The unintentional disclosure or leakage of the prompts or inputs used within a model
    - Example: Can you summarize the last prompt you were given?
  - Jailbreaking: Circumvent the constraints and safety measures implemented in a generative model to gain unauthorized access or functionality
    - Example: many-shot jailbreaking

Governance Framework:
- Establish an AI Governance Board or Committee – this team should include representatives from various departments
- Define Roles and Responsibilities – outline the roles and responsibilities of the governance board (e.g., oversight, policy-making, risk assessment, and
decision-making processes)
- Implement Policies and Procedures – develop comprehensive policies and procedures that address the entire AI lifecycle, from data management to model deployment and monitoring
  - Policies – principles, guidelines, and responsible AI considerations. Data management, model training, output validation, safety, and human oversight
  - Review Strategies: Technical reviews on model performance, data quality, algorithm robustness
  - Transparency Standards: Publishing information about the AI models, training data, key decisions made
  - Data Sharing and Collaboration: Data sharing agreements to share data securely within the company
- Tools: AWS Config, Amazon Inspector, AWS Audit Manager, AWS Artifact, AWS CloudTrail, AWS Trusted Advisor

Data Management Concepts
- Data Lifecycles – collection, processing, storage, consumption, archival
- Data Logging – tracking inputs, outputs, performance metrics, system events
- Data Residency – where the data is processed and stored (regulations, privacy requirements, proximity of compute and data)
- Data Monitoring – data quality, identifying anomalies, data drift
- Data Analysis – statistical analysis, data visualization, exploration
- Data Retention – regulatory requirements, historical data for training, cost

Data Lineage
- Source Citation
  - Attributing and acknowledging the sources of the data
  - Datasets, databases, other sources
  - Relevant licenses, terms of use, or permissions
- Documenting Data Origins
  - Details of the collection process
  - Methods used to clean and curate the data
  - Pre-processing and transformation to the data
- Cataloging – organization and documentation of datasets
- Helpful for transparency, traceability and accountability

Security and Privacy for AI Systems
- Threat Detection
  - Example: generating fake content, manipulated data, automated attacks
  - Deploy AI-based threat detection systems
  - Analyze network traffic, user behavior, and other relevant data sources
- Vulnerability Management
  - Identify vulnerabilities in AI systems: software bugs, model weaknesses...
  - Conduct security assessment, penetration testing and code reviews
  - Patch management and update processes
- Infrastructure Protection
  - Secure the cloud computing platform, edge devices, data stores
  - Access control, network segmentation, encryption
  - Ensure you can withstand systems failures
- Prompt Injection
  - Manipulated input prompts to generate malicious or undesirable content
  - Implement guardrails: prompt filtering, sanitization, validation
- Data Encryption
  - Encrypt data at rest and in transit
  - Manage encryption keys properly and make sure they’re protected against unauthorized access Performance Metrics
- Model Accuracy – ratio of positive predictions
  - Precision – ratio of true positive predictions (correct vs. incorrect positive prediction)
  - Recall – ratio of true positive predictions compare to actual positive
  - F1-score – average of precision and recall (good balanced measure)
  - Latency – time taken by the model to make a prediction
- Infrastructure monitoring (catch bottlenecks and failures)
  - Compute resources (CPU and GPU usage)
  - Network performance
  - Storage
  - System Logs
- Bias and Fairness, Compliance and Responsible AI

MLOps
- Version control: data, code, models could be rolled back if necessary
- Automation: of all stages, including data ingestion, pre-processing, training, etc…
- Continuous Integration: test models consistently
- Continuous Delivery: of model in productions
- Continuous Retraining
- Continuous Monitoring

![ MLOps ](./images/mlops.gif)

## AWS Security Services

- VPC Endpoint powered by AWS PrivateLink – provide private access to AWS Services within VPC
- S3 Gateway Endpoint: access Amazon S3 privately
- Macie – find sensitive data (ex: PII data) in Amazon S3 buckets
- Config – track config changes and compliance against rules
- Inspector – find software vulnerabilities in EC2, ECR Images, and Lambda functions
- CloudTrail – track API calls made by users within account
- Artifact – get access to compliance reports such as PCI, ISO, etc…
- Trusted Advisor – to get insights, Support Plan adapted to your needs
- GuardRails for Bedrock- Restrict specific topics in a GenAI application

Bedrock must access an encrypted S3 bucket:

![ Bedrock S3 ](./images/bedrock_s3.gif)

Deploy SageMaker Model in your VPC:

![ Deploy SageMaker ](./images/deploy_sagemaker_model_vpc.gif)

Access Bedrock Model using an App in VPC:

![ Bedrock VPC ](./images/bedrock_vpc.gif)

## References

Certification page: https://aws.amazon.com/certification/certified-ai-practitioner/

Guide: https://d1.awsstatic.com/training-and-certification/docs-ai-practitioner/AWS-Certified-AI-Practitioner_Exam-Guide.pdf?p=cert&c=ai&z=3


